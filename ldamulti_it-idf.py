import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import joblib
import re
import unicodedata

class SupervisedTopicLabeler:
    """
    Hybrid topic labeler that learns from:
    1. Manually labeled data (high quality)
    2. Keyword-based labels with high confidence (semi-supervised)
    Falls back to keyword matching for low-confidence predictions.
    """
    
    def __init__(self):
        self.topic_labels = {
            0: 'Economic Development',
            1: 'Natural Resources & Energy',
            2: 'War & Conflict',
            3: 'Social Services',
            4: 'Politics & Governance',
            5: 'Art, Technology and Sport'
        }
        
        # ML model pipeline
        self.model = None
        self.trained = False
        self.metrics = {}
        
        self.multilingual_keywords = {
            'Economic Development': {
                'en': ['economy', 'economic', 'trade', 'trading', 'investment', 'invest', 'investor',
                'business', 'market', 'finance', 'financial', 'bank', 'banking',
                'growth', 'gdp', 'currency', 'forex', 'stock', 'bonds', 'capital',
                'entrepreneur', 'commerce', 'export', 'import', 'revenue', 'profit',
                'fiscal', 'monetary', 'inflation', 'debt', 'loan', 'credit', 'development',
                'corporate', 'sector', 'commodity', 'portfolio', 'startup', 'venture', 'aid'],
                'fr': ['√©conomie', '√©conomique', 'commerce', 'investissement', 'investir', 'investisseur',
                'affaires', 'march√©', 'finance', 'financier', 'banque', 'bancaire',
                'croissance', 'pib', 'devise', 'bourse', 'obligations', 'capital',
                'entrepreneur', 'exportation', 'importation', 'revenu', 'profit',
                'fiscal', 'mon√©taire', 'inflation', 'dette', 'pr√™t', 'cr√©dit', 'd√©veloppement',
                'entreprise', 'secteur', 'marchandise', 'portefeuille', 'startup', 'aide'],
                'ar': ['ÿßŸÇÿ™ÿµÿßÿØ', 'ÿßŸÇÿ™ÿµÿßÿØŸä', 'ÿ™ÿ¨ÿßÿ±ÿ©', 'ÿßÿ≥ÿ™ÿ´ŸÖÿßÿ±', 'ÿ£ÿπŸÖÿßŸÑ', 'ÿ≥ŸàŸÇ', 'ŸÖÿßŸÑŸäÿ©', 'ÿ®ŸÜŸÉ',
                'ŸÜŸÖŸà', 'ÿπŸÖŸÑÿ©', 'ÿ±ÿ£ÿ≥ŸÖÿßŸÑ', 'ÿ±ÿ®ÿ≠', 'ÿØÿÆŸÑ', 'ÿ™ÿµÿØŸäÿ±', 'ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ',
                'ÿØŸäŸàŸÜ', 'ŸÇÿ±ÿ∂', 'ÿßÿ¶ÿ™ŸÖÿßŸÜ', 'ÿ™ÿ∂ÿÆŸÖ', 'ŸÇÿ∑ÿßÿπ', 'ÿ¥ÿ±ŸÉÿ©', 'ŸÖÿ≠ŸÅÿ∏ÿ©', 'ÿ™ÿ∑ŸàŸäÿ±'],
                'zh-cn': ['ÁªèÊµé', 'ÁªèÊµéÁöÑ', 'Ë¥∏Êòì', 'ÊäïËµÑ', 'ÂïÜ‰∏ö', 'Â∏ÇÂú∫', 'ÈáëËûç', 'Èì∂Ë°å', 'Â¢ûÈïø', 'gdp', 'Ë¥ßÂ∏Å', 'Â§ñÊ±á', 'ËÇ°Á•®', 'ÂÄ∫Âà∏', 'ËµÑÊú¨', '‰ºÅ‰∏ö', 'Âá∫Âè£', 'ËøõÂè£', 'Êî∂ÂÖ•', 'Âà©Ê∂¶', 'ÈÄöË¥ßËÜ®ËÉÄ', 'ÂÄ∫Âä°', 'Ë¥∑Ê¨æ', '‰ø°Ë¥∑', 'ÂèëÂ±ï', 'Ë°å‰∏ö', 'ÂïÜÂìÅ', 'Âàõ‰∏ö', 'ÂàùÂàõ', 'È£éÊäï', 'Êè¥Âä©'],
                'ko': ['Í≤ΩÏ†ú', 'Î¨¥Ïó≠', 'Ìà¨Ïûê', 'ÏÇ¨ÏóÖ', 'ÏãúÏû•', 'Í∏àÏúµ', 'ÏùÄÌñâ', 'ÏÑ±Ïû•', 'gdp', 'ÌÜµÌôî', 'Ïô∏Ìôò', 'Ï£ºÏãù', 'Ï±ÑÍ∂å', 'ÏûêÎ≥∏', 'Í∏∞ÏóÖ', 'ÏàòÏ∂ú', 'ÏàòÏûÖ', 'ÏàòÏùµ', 'Ïù¥Ïùµ', 'Ïù∏ÌîåÎ†àÏù¥ÏÖò', 'Î∂ÄÏ±Ñ', 'ÎåÄÏ∂ú', 'Ïã†Ïö©', 'Í∞úÎ∞ú', 'ÏÇ∞ÏóÖ', 'ÏÉÅÌíà', 'Ï∞ΩÏóÖ', 'Ïä§ÌÉÄÌä∏ÏóÖ', 'Î≤§Ï≤ò', 'ÏõêÏ°∞'],
                'es': ['econom√≠a', 'econ√≥mico', 'comercio', 'inversi√≥n', 'invertir', 'inversor', 'negocios', 'mercado', 'finanzas', 'financiero', 'banco', 'banca', 'crecimiento', 'pib', 'moneda', 'divisa', 'bolsa', 'bonos', 'capital', 'emprendedor', 'exportaci√≥n', 'importaci√≥n', 'ingresos', 'beneficio', 'fiscal', 'monetario', 'inflaci√≥n', 'deuda', 'pr√©stamo', 'cr√©dito', 'desarrollo', 'empresa', 'sector', 'mercanc√≠a', 'cartera', 'startup', 'ayuda'],
                'pt': ['economia', 'econ√¥mico', 'com√©rcio', 'investimento', 'investir', 'investidor', 'neg√≥cios', 'mercado', 'finan√ßas', 'financeiro', 'banco', 'banc√°rio', 'crescimento', 'pib', 'moeda', 'c√¢mbio', 'bolsa', 't√≠tulos', 'capital', 'empreendedor', 'exporta√ß√£o', 'importa√ß√£o', 'receita', 'lucro', 'fiscal', 'monet√°rio', 'infla√ß√£o', 'd√≠vida', 'empr√©stimo', 'cr√©dito', 'desenvolvimento', 'empresa', 'setor', 'mercadoria', 'carteira', 'startup', 'ajuda'],
            },
            'Natural Resources & Energy': {
                'en': ['oil', 'petroleum', 'crude', 'mining', 'mineral', 'coal', 'gold',
                'diamond', 'energy', 'power', 'electricity', 'gas', 'natural gas',
                'renewable', 'solar', 'wind', 'hydro', 'nuclear', 'extraction',
                'resources', 'fossil', 'lithium', 'copper', 'cobalt', 'uranium',
                'refinery', 'drilling', 'pipeline', 'reserves', 'exploration',
                'hydroelectric', 'fuel', 'ore', 'geothermal', 'biomass', 'turbine', 'grid'],
                'fr': ['p√©trole', 'brut', 'exploitation mini√®re', 'mine', 'min√©ral', 'charbon', 'or',
                'diamant', '√©nergie', '√©lectricit√©', 'gaz', 'gaz naturel',
                'renouvelable', 'solaire', 'vent', '√©olien', 'hydro', 'hydro√©lectrique', 'nucl√©aire', 'extraction',
                'ressources', 'fossile', 'lithium', 'cuivre', 'cobalt', 'uranium',
                'raffinerie', 'forage', 'pipeline', 'r√©serves', 'exploration',
                'carburant', 'minerai', 'g√©othermique', 'biomasse', 'turbine', 'r√©seau'],
                'ar': ['ŸÜŸÅÿ∑', 'ÿ≤Ÿäÿ™', 'ÿ™ÿπÿØŸäŸÜ', 'ŸÖÿπÿØŸÜ', 'ŸÅÿ≠ŸÖ', 'ÿ∞Ÿáÿ®',
                'ÿ£ŸÑŸÖÿßÿ≥', 'ÿ∑ÿßŸÇÿ©', 'ŸÉŸáÿ±ÿ®ÿßÿ°', 'ÿ∫ÿßÿ≤',
                'ŸÖÿ™ÿ¨ÿØÿØÿ©', 'ÿ¥ŸÖÿ≥Ÿä', 'ÿ±Ÿäÿßÿ≠', 'ŸÖÿßÿ¶Ÿä', 'ŸÜŸàŸàŸä', 'ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨',
                'ŸÖŸàÿßÿ±ÿØ', 'ŸàŸÇŸàÿØ', 'ŸÑŸäÿ´ŸäŸàŸÖ', 'ŸÜÿ≠ÿßÿ≥', 'ŸäŸàÿ±ÿßŸÜŸäŸàŸÖ',
                'ÿ™ŸÉÿ±Ÿäÿ±', 'ÿ≠ŸÅÿ±', 'ÿÆÿ∑', 'ÿßÿ≠ÿ™Ÿäÿßÿ∑Ÿäÿßÿ™', 'ÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ',
                'ÿÆÿßŸÖÿßÿ™', 'ÿ≠ÿ±ÿßÿ±Ÿä', 'ÿ≠ŸäŸàŸä', 'ÿ™Ÿàÿ±ÿ®ŸäŸÜ', 'ÿ¥ÿ®ŸÉÿ©'],
                'zh-cn': ['Áü≥Ê≤π', 'ÂéüÊ≤π', 'ÈááÁüø', 'Áüø‰∫ß', 'ÁÖ§ÁÇ≠', 'ÈªÑÈáë', 'ÈíªÁü≥', 'ËÉΩÊ∫ê', 'ÁîµÂäõ', 'Â§©ÁÑ∂Ê∞î', 'ÂèØÂÜçÁîü', 'Â§™Èò≥ËÉΩ', 'È£éËÉΩ', 'Ê∞¥Áîµ', 'Ê†∏ËÉΩ', 'ÂºÄÈáá', 'ËµÑÊ∫ê', 'ÂåñÁü≥', 'ÈîÇ', 'Èìú', 'Èí¥', 'ÈìÄ', 'ÁÇºÊ≤πÂéÇ', 'ÈíªÊé¢', 'ÁÆ°ÈÅì', 'ÂÇ®Â§á', 'ÂãòÊé¢', 'ÁáÉÊñô', 'ÁüøÁü≥', 'Âú∞ÁÉ≠', 'ÁîüÁâ©Ë¥®', 'Ê∂°ËΩÆÊú∫', 'ÁîµÁΩë'],
                'ko': ['ÏÑùÏú†', 'ÏõêÏú†', 'Ï±ÑÍµ¥', 'Í¥ëÎ¨º', 'ÏÑùÌÉÑ', 'Í∏à', 'Îã§Ïù¥ÏïÑÎ™¨Îìú', 'ÏóêÎÑàÏßÄ', 'Ï†ÑÎ†•', 'Ï≤úÏó∞Í∞ÄÏä§', 'Ïû¨ÏÉùÏóêÎÑàÏßÄ', 'ÌÉúÏñëÍ¥ë', 'ÌíçÎ†•', 'ÏàòÎ†•', 'ÏõêÏûêÎ†•', 'Ï±ÑÏ∑®', 'ÏûêÏõê', 'ÌôîÏÑù', 'Î¶¨Ìä¨', 'Íµ¨Î¶¨', 'ÏΩîÎ∞úÌä∏', 'Ïö∞ÎùºÎäÑ', 'Ï†ïÏú†', 'ÏãúÏ∂î', 'ÌååÏù¥ÌîÑÎùºÏù∏', 'Îß§Ïû•Îüâ', 'ÌÉêÏÇ¨', 'Ïó∞Î£å', 'Í¥ëÏÑù', 'ÏßÄÏó¥', 'Î∞îÏù¥Ïò§Îß§Ïä§', 'ÌÑ∞Îπà', 'Ï†ÑÎ†•Îßù'],
                'es': ['petr√≥leo', 'crudo', 'miner√≠a', 'mineral', 'carb√≥n', 'oro', 'diamante', 'energ√≠a', 'electricidad', 'gas', 'gas natural', 'renovable', 'solar', 'e√≥lica', 'hidroel√©ctrica', 'nuclear', 'extracci√≥n', 'recursos', 'f√≥sil', 'litio', 'cobre', 'cobalto', 'uranio', 'refiner√≠a', 'perforaci√≥n', 'oleoducto', 'reservas', 'exploraci√≥n', 'combustible', 'mena', 'geot√©rmica', 'biomasa', 'turbina', 'red'],
                'pt': ['petr√≥leo', 'bruto', 'minera√ß√£o', 'mineral', 'carv√£o', 'ouro', 'diamante', 'energia', 'eletricidade', 'g√°s', 'g√°s natural', 'renov√°vel', 'solar', 'e√≥lica', 'hidrel√©trica', 'nuclear', 'extra√ß√£o', 'recursos', 'f√≥ssil', 'l√≠tio', 'cobre', 'cobalto', 'ur√¢nio', 'refinaria', 'perfura√ß√£o', 'oleoduto', 'reservas', 'explora√ß√£o', 'combust√≠vel', 'min√©rio', 'geot√©rmica', 'biomassa', 'turbina', 'rede'],
            },
            'War & Conflict': {
                'en': ['war', 'conflict', 'violence', 'military', 'army', 'soldier', 'troop',
                'rebel', 'insurgent', 'militant', 'attack', 'bombing', 'strike',
                'terrorism', 'terrorist', 'extremist', 'jihadist', 'boko haram',
                'al-shabaab', 'militia', 'armed group', 'peacekeeping', 'ceasefire',
                'casualties', 'killed', 'wounded', 'battle', 'fighting', 'clash',
                'combat', 'offensive', 'raid', 'ambush', 'siege', 'refugee',
                'displacement', 'humanitarian crisis', 'genocide', 'ethnic cleansing',
                'civil war', 'coup', 'rebellion', 'uprising', 'unrest', 'protest violence',
                'security forces', 'defense', 'weapon', 'arms', 'ammunition',
                'warfare', 'death', 'kill', 'guns', 'gun', 'bomb', 'dead', 'bodies', 'fire',
                'fight', 'violent', 'assault', 'airstrike', 'shelling', 'hostilities'],
                'fr': ['guerre', 'conflit', 'violence', 'militaire', 'arm√©e', 'soldat', 'troupe',
                'rebelle', 'insurg√©', 'militant', 'attaque', 'bombardement', 'frappe',
                'terrorisme', 'terroriste', 'extr√©miste', 'milice', 'cessez-le-feu',
                'victimes', 'tu√©s', 'tu√©', 'bless√©s', 'bataille', 'combat', 'combats',
                'offensive', 'raid', 'si√®ge', 'r√©fugi√©',
                'crise humanitaire', 'g√©nocide',
                'guerre civile', 'coup', 'r√©bellion', 'soul√®vement',
                'forces de s√©curit√©', 'd√©fense', 'arme', 'armes', 'munitions',
                'mort', 'tuer', 'fusil', 'bombe', 'morts', 'corps', 'feu',
                'violent', 'assaut', 'frappe a√©rienne', 'pilonnage', 'hostilit√©s'],
                'ar': ['ÿ≠ÿ±ÿ®', 'ÿµÿ±ÿßÿπ', 'ŸÜÿ≤ÿßÿπ', 'ÿπŸÜŸÅ', 'ÿπÿ≥ŸÉÿ±Ÿä', 'ÿ¨Ÿäÿ¥', 'ÿ¨ŸÜÿØŸä', 'ŸÇŸàÿßÿ™',
                'ŸÖÿ™ŸÖÿ±ÿØ', 'ŸÖÿ™ÿ¥ÿØÿØ', 'Ÿáÿ¨ŸàŸÖ', 'ŸÇÿµŸÅ', 'ÿ∂ÿ±ÿ®ÿ©',
                'ÿ•ÿ±Ÿáÿßÿ®', 'ÿ•ÿ±Ÿáÿßÿ®Ÿä', 'ŸÖÿ™ÿ∑ÿ±ŸÅ', 'ŸÖŸäŸÑŸäÿ¥Ÿäÿß', 'ŸáÿØŸÜÿ©',
                'ÿ∂ÿ≠ÿßŸäÿß', 'ŸÇÿ™ŸÑŸâ', 'ŸÖŸÇÿ™ŸàŸÑ', 'ÿ¨ÿ±ÿ≠Ÿâ', 'ŸÖÿπÿ±ŸÉÿ©', 'ŸÇÿ™ÿßŸÑ',
                'Ÿáÿ¨ŸàŸÖ', 'ÿ≠ÿµÿßÿ±', 'ŸÑÿßÿ¨ÿ¶',
                'ÿ£ÿ≤ŸÖÿ© ÿ•ŸÜÿ≥ÿßŸÜŸäÿ©', 'ÿ•ÿ®ÿßÿØÿ©',
                'ÿ≠ÿ±ÿ® ÿ£ŸáŸÑŸäÿ©', 'ÿßŸÜŸÇŸÑÿßÿ®', 'ÿ´Ÿàÿ±ÿ©', 'ÿ™ŸÖÿ±ÿØ',
                'ŸÇŸàÿßÿ™ ÿßŸÑÿ£ŸÖŸÜ', 'ÿØŸÅÿßÿπ', 'ÿ≥ŸÑÿßÿ≠', 'ÿ£ÿ≥ŸÑÿ≠ÿ©', 'ÿ∞ÿÆŸäÿ±ÿ©',
                'ŸÖŸàÿ™', 'ŸÇÿ™ŸÑ', 'ÿ®ŸÜÿØŸÇŸäÿ©', 'ŸÇŸÜÿ®ŸÑÿ©', 'ÿ¨ÿ´ÿ©', 'ÿ¨ÿ´ÿ´', 'ŸÜÿßÿ±',
                'ÿπÿØŸàÿßŸÜ', 'ŸÖÿ≥ŸÑÿ≠', 'ÿπÿØÿßŸàÿ©']
                ,
                'zh-cn': ['Êàò‰∫â', 'ÂÜ≤Á™Å', 'Êö¥Âäõ', 'ÂÜõ‰∫ã', 'ÂÜõÈòü', 'Â£´ÂÖµ', 'ÂèõÂÜõ', 'Ê≠¶Ë£ÖÂàÜÂ≠ê', 'ÊîªÂáª', 'ÁàÜÁÇ∏', 'Á©∫Ë¢≠', 'ÊÅêÊÄñ‰∏ª‰πâ', 'ÊÅêÊÄñÂàÜÂ≠ê', 'ÊûÅÁ´ØÂàÜÂ≠ê', 'Ê∞ëÂÖµ', '‰ºëÊàò', '‰º§‰∫°', 'Ê≠ª‰∫°', 'Âèó‰º§', 'ÊàòÊñó', '‰∫§Êàò', '‰ΩúÊàò', 'ËøõÊîª', 'Á™ÅË¢≠', '‰ºèÂáª', 'Âõ¥Âõ∞', 'ÈöæÊ∞ë', 'ÊµÅÁ¶ªÂ§±ÊâÄ', '‰∫∫ÈÅìÂç±Êú∫', 'ÁßçÊóèÁÅ≠Áªù', 'ÂÜÖÊàò', 'ÊîøÂèò', 'Âèõ‰π±', 'Ëµ∑‰πâ', 'Âä®‰π±', 'ÊäóËÆÆ', 'ÂÆâÂÖ®ÈÉ®Èòü', 'Èò≤Âæ°', 'Ê≠¶Âô®', 'ÂºπËçØ', 'Êû™ÊîØ', 'ÁÇ∏Âºπ', 'ÊïåÂØπ'],
                'ko': ['Ï†ÑÏüÅ', 'Î∂ÑÏüÅ', 'Ìè≠Î†•', 'Íµ∞ÏÇ¨', 'Íµ∞ÎåÄ', 'Î≥ëÏÇ¨', 'Î∞òÍµ∞', 'Î¨¥Ïû•Îã®Ï≤¥', 'Í≥µÍ≤©', 'Ìè≠ÌÉÑ', 'Í≥µÏäµ', 'ÌÖåÎü¨', 'ÌÖåÎü¨Î¶¨Ïä§Ìä∏', 'Í∑πÎã®Ï£ºÏùòÏûê', 'ÎØºÎ≥ëÎåÄ', 'Ìú¥Ï†Ñ', 'ÏÇ¨ÏÉÅÏûê', 'ÏÇ¨Îßù', 'Î∂ÄÏÉÅ', 'Ï†ÑÌà¨', 'ÍµêÏ†Ñ', 'ÏûëÏ†Ñ', 'Í≥µÏÑ∏', 'Í∏âÏäµ', 'Îß§Î≥µ', 'Ìè¨ÏúÑ', 'ÎÇúÎØº', 'Ïù∏ÎèÑÏ†Å ÏúÑÍ∏∞', 'ÏßëÎã®ÌïôÏÇ¥', 'ÎÇ¥Ï†Ñ', 'Ïø†Îç∞ÌÉÄ', 'Î∞òÎûÄ', 'Î¥âÍ∏∞', 'ÏÜåÏöî', 'ÏãúÏúÑ', 'ÏπòÏïàÍµ∞', 'Î∞©Ïñ¥', 'Î¨¥Í∏∞', 'ÌÉÑÏïΩ', 'Ï¥ùÍ∏∞', 'Ìè≠Î∞úÎ¨º', 'Ï†ÅÎåÄ'],
                'es': ['guerra', 'conflicto', 'violencia', 'militar', 'ej√©rcito', 'soldado', 'rebelde', 'insurgente', 'militante', 'ataque', 'bombardeo', 'ofensiva', 'terrorismo', 'terrorista', 'extremista', 'milicia', 'alto el fuego', 'v√≠ctimas', 'muertos', 'heridos', 'batalla', 'combates', 'choque', 'combate', 'incursi√≥n', 'emboscada', 'asedio', 'refugiado', 'desplazamiento', 'crisis humanitaria', 'genocidio', 'guerra civil', 'golpe', 'rebeli√≥n', 'levantamiento', 'disturbios', 'protesta', 'fuerzas de seguridad', 'defensa', 'arma', 'armas', 'munici√≥n'],
                'pt': ['guerra', 'conflito', 'viol√™ncia', 'militar', 'ex√©rcito', 'soldado', 'rebelde', 'insurgente', 'militante', 'ataque', 'bombardeio', 'ofensiva', 'terrorismo', 'terrorista', 'extremista', 'mil√≠cia', 'cessar-fogo', 'v√≠timas', 'mortos', 'feridos', 'batalha', 'combates', 'confronto', 'opera√ß√£o', 'incurs√£o', 'emboscada', 'cerco', 'refugiado', 'deslocamento', 'crise humanit√°ria', 'genoc√≠dio', 'guerra civil', 'golpe', 'rebeli√£o', 'levante', 'dist√∫rbios', 'protesto', 'for√ßas de seguran√ßa', 'defesa', 'arma', 'armas', 'muni√ß√£o']
            },
            'Social Services': {
                'en': ['health', 'healthcare', 'hospital', 'medical', 'doctor', 'nurse', 'physician',
                'clinic', 'patient', 'disease', 'vaccine', 'medicine', 'treatment', 'therapy',
                'education', 'school', 'university', 'teacher', 'student', 'learning',
                'training', 'literacy', 'scholarship', 'welfare', 'social',
                'pandemic', 'epidemic', 'curriculum', 'tuition', 'degree','covid','schools',
                'virus', 'malaria', 'tuberculosis', 'hiv', 'aids', 'immunization',
                'maternal', 'childcare', 'hunger', 'nutrition', 'water', 'sanitation', 'shelter'],
                'fr': ['sant√©', 'soins', 'h√¥pital', 'm√©dical', 'm√©decin', 'infirmier', 'infirmi√®re',
                'clinique', 'patient', 'maladie', 'vaccin', 'm√©dicament', 'traitement', 'th√©rapie',
                '√©ducation', '√©cole', 'universit√©', 'enseignant', 'professeur', '√©tudiant', 'apprentissage',
                'formation', 'alphab√©tisation', 'bourse', 'bien-√™tre', 'social',
                'pand√©mie', '√©pid√©mie', 'programme', 'dipl√¥me', 'covid',
                'virus', 'paludisme', 'tuberculose', 'vih', 'sida', 'immunisation',
                'maternelle', 'garde', 'faim', 'nutrition', 'eau', 'assainissement', 'abri'],
                'ar': ['ÿµÿ≠ÿ©', 'ÿ±ÿπÿßŸäÿ©', 'ŸÖÿ≥ÿ™ÿ¥ŸÅŸâ', 'ÿ∑ÿ®Ÿä', 'ÿ∑ÿ®Ÿäÿ®', 'ŸÖŸÖÿ±ÿ∂ÿ©',
                'ÿπŸäÿßÿØÿ©', 'ŸÖÿ±Ÿäÿ∂', 'ŸÖÿ±ÿ∂', 'ŸÑŸÇÿßÿ≠', 'ÿØŸàÿßÿ°', 'ÿπŸÑÿßÿ¨',
                'ÿ™ÿπŸÑŸäŸÖ', 'ŸÖÿØÿ±ÿ≥ÿ©', 'ÿ¨ÿßŸÖÿπÿ©', 'ŸÖÿπŸÑŸÖ', 'ÿ∑ÿßŸÑÿ®', 'ÿ™ÿπŸÑŸÖ',
                'ÿ™ÿØÿ±Ÿäÿ®', 'ŸÖÿ≠Ÿà ÿßŸÑÿ£ŸÖŸäÿ©', 'ŸÖŸÜÿ≠ÿ©', 'ÿ±ÿπÿßŸäÿ©', 'ÿßÿ¨ÿ™ŸÖÿßÿπŸä',
                'ÿ¨ÿßÿ¶ÿ≠ÿ©', 'Ÿàÿ®ÿßÿ°', 'ŸÖŸÜŸáÿ¨', 'ÿØÿ±ÿ¨ÿ©', 'ŸÉŸàŸÅŸäÿØ',
                'ŸÅŸäÿ±Ÿàÿ≥', 'ŸÖŸÑÿßÿ±Ÿäÿß', 'ÿ≥ŸÑ', 'ÿ•ŸäÿØÿ≤', 'ÿ™ÿ≠ÿµŸäŸÜ',
                'ÿ£ŸÖŸàŸÖÿ©', 'ÿ¨Ÿàÿπ', 'ÿ™ÿ∫ÿ∞Ÿäÿ©', 'ŸÖÿßÿ°', 'ÿµÿ±ŸÅ', 'ŸÖŸÑÿ¨ÿ£']
                ,
                'zh-cn': ['ÂÅ•Â∫∑', 'ÂåªÁñó', 'ÂåªÈô¢', 'ÂåªÁîü', 'Êä§Â£´', 'ËØäÊâÄ', 'ÊÇ£ËÄÖ', 'ÁñæÁóÖ', 'Áñ´Ëãó', 'ËçØÁâ©', 'Ê≤ªÁñó', 'ÁñóÊ≥ï', 'ÊïôËÇ≤', 'Â≠¶Ê†°', 'Â§ßÂ≠¶', 'ÊïôÂ∏à', 'Â≠¶Áîü', 'Â≠¶‰π†', 'ÂüπËÆ≠', 'ËØÜÂ≠ó', 'Â•ñÂ≠¶Èáë', 'Á¶èÂà©', 'Á§æ‰ºö', 'Â§ßÊµÅË°å', 'ÊµÅË°åÁóÖ', 'ËØæÁ®ã', 'Â≠¶Ë¥π', 'Â≠¶‰Ωç', 'Êñ∞ÂÜ†', 'ÁóÖÊØí', 'ÁñüÁñæ', 'ÁªìÊ†∏ÁóÖ', 'ËâæÊªãÁóÖ', 'ÂÖçÁñ´Êé•Áßç', 'ÊØçÂ©¥', 'ÂÑøÁ´•Êä§ÁêÜ', 'È••È•ø', 'Ëê•ÂÖª', 'Ê∞¥', 'Âç´Áîü', '‰ΩèÊâÄ'],
                'ko': ['Í±¥Í∞ï', 'ÏùòÎ£å', 'Î≥ëÏõê', 'ÏùòÏÇ¨', 'Í∞ÑÌò∏ÏÇ¨', 'ÌÅ¥Î¶¨Îãâ', 'ÌôòÏûê', 'ÏßàÎ≥ë', 'Î∞±Ïã†', 'ÏïΩ', 'ÏπòÎ£å', 'ÏöîÎ≤ï', 'ÍµêÏú°', 'ÌïôÍµê', 'ÎåÄÌïô', 'ÍµêÏÇ¨', 'ÌïôÏÉù', 'ÌïôÏäµ', 'ÌõàÎ†®', 'Î¨∏Ìï¥', 'Ïû•ÌïôÍ∏à', 'Î≥µÏßÄ', 'ÏÇ¨Ìöå', 'Ìå¨Îç∞ÎØπ', 'Ï†ÑÏóºÎ≥ë', 'ÍµêÏú°Í≥ºÏ†ï', 'Îì±Î°ùÍ∏à', 'ÌïôÏúÑ', 'ÏΩîÎ°úÎÇò', 'Î∞îÏù¥Îü¨Ïä§', 'ÎßêÎùºÎ¶¨ÏïÑ', 'Í≤∞Ìïµ', 'HIV', 'ÏóêÏù¥Ï¶à', 'ÏòàÎ∞©Ï†ëÏ¢Ö', 'ÏÇ∞Î™®', 'Î≥¥Ïú°', 'Íµ∂Ï£ºÎ¶º', 'ÏòÅÏñë', 'Î¨º', 'ÏúÑÏÉù', 'Ï£ºÍ±∞'],
                'es': ['salud', 'atenci√≥n', 'hospital', 'm√©dico', 'doctor', 'enfermera', 'cl√≠nica', 'paciente', 'enfermedad', 'vacuna', 'medicamento', 'tratamiento', 'terapia', 'educaci√≥n', 'escuela', 'universidad', 'maestro', 'profesor', 'estudiante', 'aprendizaje', 'formaci√≥n', 'alfabetizaci√≥n', 'beca', 'bienestar', 'social', 'pandemia', 'epidemia', 'curr√≠culo', 'matr√≠cula', 't√≠tulo', 'covid', 'virus', 'malaria', 'tuberculosis', 'vih', 'sida', 'inmunizaci√≥n', 'materno', 'guarder√≠a', 'hambre', 'nutrici√≥n', 'agua', 'saneamiento', 'refugio'],
                'pt': ['sa√∫de', 'cuidados', 'hospital', 'm√©dico', 'doutor', 'enfermeira', 'cl√≠nica', 'paciente', 'doen√ßa', 'vacina', 'medicamento', 'tratamento', 'terapia', 'educa√ß√£o', 'escola', 'universidade', 'professor', 'estudante', 'aprendizagem', 'forma√ß√£o', 'alfabetiza√ß√£o', 'bolsa', 'bem-estar', 'social', 'pandemia', 'epidemia', 'curr√≠culo', 'propina', 'diploma', 'covid', 'v√≠rus', 'mal√°ria', 'tuberculose', 'vih', 'sida', 'imuniza√ß√£o', 'materno', 'creche', 'fome', 'nutri√ß√£o', '√°gua', 'saneamento', 'abrigo']
            },
            
            'Politics & Governance': {
                'en': ['politics', 'political', 'government', 'governance', 'president',
                'minister', 'parliament', 'congress', 'election', 'vote', 'voting',
                'democracy', 'policy', 'law', 'legislation', 'regulation', 'cabinet',
                'opposition', 'party', 'campaign', 'referendum', 'constitution',
                'diplomacy', 'treaty', 'summit', 'senator', 'governor', 'mayor','administration',
                'prime minister', 'reform', 'ruling', 'rebellion', 'coup',
                'senate', 'judiciary', 'ballot', 'diplomat', 'sanctions', 'sovereignty'],
                'fr': ['politique', 'gouvernement', 'gouvernance', 'pr√©sident',
                'ministre', 'parlement', 'congr√®s', '√©lection', 'vote', 'scrutin',
                'd√©mocratie', 'politique', 'loi', 'l√©gislation', 'r√®glement', 'cabinet',
                'opposition', 'parti', 'campagne', 'r√©f√©rendum', 'constitution',
                'diplomatie', 'trait√©', 'sommet', 's√©nateur', 'gouverneur', 'maire', 'administration',
                'premier ministre', 'r√©forme', 'r√®gne', 'r√©bellion', 'coup',
                's√©nat', 'judiciaire', 'diplomate', 'sanctions', 'souverainet√©'],
                'ar': ['ÿ≥Ÿäÿßÿ≥ÿ©', 'ÿ≥Ÿäÿßÿ≥Ÿä', 'ÿ≠ŸÉŸàŸÖÿ©', 'ÿ≠ŸÉŸÖ', 'ÿ±ÿ¶Ÿäÿ≥',
                'Ÿàÿ≤Ÿäÿ±', 'ÿ®ÿ±ŸÑŸÖÿßŸÜ', 'ŸÖÿ¨ŸÑÿ≥', 'ÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™', 'ÿ™ÿµŸàŸäÿ™',
                'ÿØŸäŸÖŸÇÿ±ÿßÿ∑Ÿäÿ©', 'ŸÇÿßŸÜŸàŸÜ', 'ÿ™ÿ¥ÿ±Ÿäÿπ', 'ŸÇÿßŸÜŸàŸÜ', 'Ÿàÿ≤ÿ±ÿßÿ°',
                'ŸÖÿπÿßÿ±ÿ∂ÿ©', 'ÿ≠ÿ≤ÿ®', 'ÿ≠ŸÖŸÑÿ©', 'ÿßÿ≥ÿ™ŸÅÿ™ÿßÿ°', 'ÿØÿ≥ÿ™Ÿàÿ±',
                'ÿØÿ®ŸÑŸàŸÖÿßÿ≥Ÿäÿ©', 'ŸÖÿπÿßŸáÿØÿ©', 'ŸÇŸÖÿ©', 'ÿ≠ÿßŸÉŸÖ', 'ÿπŸÖÿØÿ©', 'ÿ•ÿØÿßÿ±ÿ©',
                'Ÿàÿ≤Ÿäÿ± ÿ£ŸàŸÑ', 'ÿ•ÿµŸÑÿßÿ≠', 'ÿ≠ŸÉŸÖ', 'ÿ´Ÿàÿ±ÿ©', 'ÿßŸÜŸÇŸÑÿßÿ®',
                'ŸÇÿ∂ÿßÿ¶Ÿä', 'ÿØÿ®ŸÑŸàŸÖÿßÿ≥Ÿä', 'ÿπŸÇŸàÿ®ÿßÿ™', 'ÿ≥ŸäÿßÿØÿ©'],
                'zh-cn': ['ÊîøÊ≤ª', 'ÊîøÂ∫ú', 'Ê≤ªÁêÜ', 'ÊÄªÁªü', 'ÈÉ®Èïø', 'ËÆÆ‰ºö', 'ÂõΩ‰ºö', 'ÈÄâ‰∏æ', 'ÊäïÁ•®', 'Ê∞ë‰∏ª', 'ÊîøÁ≠ñ', 'Ê≥ïÂæã', 'Á´ãÊ≥ï', 'ÁõëÁÆ°', 'ÂÜÖÈòÅ', 'ÂèçÂØπÊ¥æ', 'ÊîøÂÖö', 'Á´ûÈÄâ', 'ÂÖ¨Êäï', 'ÂÆ™Ê≥ï', 'Â§ñ‰∫§', 'Êù°Á∫¶', 'Â≥∞‰ºö', 'ÂèÇËÆÆÂëò', 'Â∑ûÈïø', 'Â∏ÇÈïø', 'Ë°åÊîø', 'ÊÄªÁêÜ', 'ÊîπÈù©', 'ÊâßÊîø', 'Âèõ‰π±', 'ÊîøÂèò', 'ÂèÇËÆÆÈô¢', 'Âè∏Ê≥ï', 'ÈÄâÁ•®', 'Â§ñ‰∫§ÂÆò', 'Âà∂Ë£Å', '‰∏ªÊùÉ'],
                'ko': ['Ï†ïÏπò', 'Ï†ïÎ∂Ä', 'Í±∞Î≤ÑÎÑåÏä§', 'ÎåÄÌÜµÎ†π', 'Ïû•Í¥Ä', 'ÏùòÌöå', 'Íµ≠Ìöå', 'ÏÑ†Í±∞', 'Ìà¨Ìëú', 'ÎØºÏ£ºÏ£ºÏùò', 'Ï†ïÏ±Ö', 'Î≤ïÎ•†', 'ÏûÖÎ≤ï', 'Í∑úÏ†ú', 'ÎÇ¥Í∞Å', 'ÏïºÎãπ', 'Ï†ïÎãπ', 'ÏÑ†Í±∞Ïö¥Îèô', 'Íµ≠ÎØºÌà¨Ìëú', 'ÌóåÎ≤ï', 'Ïô∏Íµê', 'Ï°∞ÏïΩ', 'Ï†ïÏÉÅÌöåÎã¥', 'ÏÉÅÏõêÏùòÏõê', 'Ï£ºÏßÄÏÇ¨', 'ÏãúÏû•', 'ÌñâÏ†ïÎ∂Ä', 'Ï¥ùÎ¶¨', 'Í∞úÌòÅ', 'ÏßëÍ∂å', 'Î∞òÎûÄ', 'Ïø†Îç∞ÌÉÄ', 'ÏÉÅÏõê', 'ÏÇ¨Î≤ïÎ∂Ä', 'Ìà¨ÌëúÏö©ÏßÄ', 'Ïô∏ÍµêÍ¥Ä', 'Ï†úÏû¨', 'Ï£ºÍ∂å'],
                'es': ['pol√≠tica', 'gobierno', 'gobernanza', 'presidente', 'ministro', 'parlamento', 'congreso', 'elecci√≥n', 'voto', 'votar', 'democracia', 'pol√≠tica p√∫blica', 'ley', 'legislaci√≥n', 'regulaci√≥n', 'gabinete', 'oposici√≥n', 'partido', 'campa√±a', 'refer√©ndum', 'constituci√≥n', 'diplomacia', 'tratado', 'cumbre', 'senador', 'gobernador', 'alcalde', 'administraci√≥n', 'primer ministro', 'reforma', 'gobernar', 'rebeli√≥n', 'golpe', 'senado', 'poder judicial', 'papeleta', 'diplom√°tico', 'sanciones', 'soberan√≠a'],
                'pt': ['pol√≠tica', 'governo', 'governan√ßa', 'presidente', 'ministro', 'parlamento', 'congresso', 'elei√ß√£o', 'voto', 'democracia', 'pol√≠tica p√∫blica', 'lei', 'legisla√ß√£o', 'regula√ß√£o', 'gabinete', 'oposi√ß√£o', 'partido', 'campanha', 'referendo', 'constitui√ß√£o', 'diplomacia', 'tratado', 'c√∫pula', 'senador', 'governador', 'prefeito', 'administra√ß√£o', 'primeiro-ministro', 'reforma', 'governar', 'rebeli√£o', 'golpe', 'senado', 'judici√°rio', 'c√©dula', 'diplomata', 'san√ß√µes', 'soberania']
            },
            'Art, Technology and Sport': {
                'en': ['art', 'artist', 'music', 'musician', 'painting', 'sculpture', 'gallery', 'museum',
                'exhibition', 'performance', 'theatre', 'theater', 'film', 'cinema', 'movie',
                'sport', 'sports', 'football', 'soccer', 'basketball', 'tennis', 'athletics',
                'athlete', 'championship', 'tournament', 'league', 'match', 'game', 'player',
                'team', 'coach', 'olympic', 'olympics', 'medal', 'victory', 'champion',
                'culture', 'cultural', 'heritage', 'festival', 'dance'],
                'fr': ['art', 'artiste', 'musique', 'musicien', 'peinture', 'sculpture', 'galerie', 'mus√©e',
                'exposition', 'spectacle', 'th√©√¢tre', 'film', 'cin√©ma',
                'sport', 'sports', 'football', 'basket', 'tennis', 'athl√©tisme',
                'athl√®te', 'championnat', 'tournoi', 'ligue', 'match', 'jeu', 'joueur',
                '√©quipe', 'entra√Æneur', 'olympique', 'olympiques', 'm√©daille', 'victoire', 'champion',
                'culture', 'culturel', 'patrimoine', 'festival', 'danse', 'comp√©tition'],
                'ar': ['ŸÅŸÜ', 'ŸÅŸÜÿßŸÜ', 'ŸÖŸàÿ≥ŸäŸÇŸâ', 'ŸÖŸàÿ≥ŸäŸÇŸä', 'ÿ±ÿ≥ŸÖ', 'ŸÜÿ≠ÿ™', 'ŸÖÿπÿ±ÿ∂', 'ŸÖÿ™ÿ≠ŸÅ',
                'ÿπÿ±ÿ∂', 'ÿ£ÿØÿßÿ°', 'ŸÖÿ≥ÿ±ÿ≠', 'ŸÅŸäŸÑŸÖ', 'ÿ≥ŸäŸÜŸÖÿß',
                'ÿ±Ÿäÿßÿ∂ÿ©', 'ÿ±Ÿäÿßÿ∂Ÿä', 'ŸÉÿ±ÿ© ŸÇÿØŸÖ', 'ŸÉÿ±ÿ© ÿ≥ŸÑÿ©', 'ÿ™ŸÜÿ≥', 'ÿ£ŸÑÿπÿßÿ® ŸÇŸàŸâ',
                'ÿ±Ÿäÿßÿ∂Ÿä', 'ÿ®ÿ∑ŸàŸÑÿ©', 'ÿØŸàÿ±Ÿä', 'ŸÖÿ®ÿßÿ±ÿßÿ©', 'ŸÑÿπÿ®ÿ©', 'ŸÑÿßÿπÿ®',
                'ŸÅÿ±ŸäŸÇ', 'ŸÖÿØÿ±ÿ®', 'ÿ£ŸàŸÑŸÖÿ®Ÿä', 'ÿ£ŸàŸÑŸÖÿ®ŸäÿßÿØ', 'ŸÖŸäÿØÿßŸÑŸäÿ©', 'ŸÅŸàÿ≤', 'ÿ®ÿ∑ŸÑ',
                'ÿ´ŸÇÿßŸÅÿ©', 'ÿ´ŸÇÿßŸÅŸä', 'ÿ™ÿ±ÿßÿ´', 'ŸÖŸáÿ±ÿ¨ÿßŸÜ', 'ÿ±ŸÇÿµ', 'ŸÖŸÜÿßŸÅÿ≥ÿ©']
                ,
                'zh-cn': ['Ëâ∫ÊúØ', 'Ëâ∫ÊúØÂÆ∂', 'Èü≥‰πê', 'Èü≥‰πêÂÆ∂', 'ÁªòÁîª', 'ÈõïÂ°ë', 'ÁîªÂªä', 'ÂçöÁâ©È¶Ü', 'Â±ïËßà', 'Ë°®Êºî', 'ÂâßÈô¢', 'ÁîµÂΩ±', 'ÂΩ±Èô¢', '‰ΩìËÇ≤', 'Ë∂≥ÁêÉ', 'ÁØÆÁêÉ', 'ÁΩëÁêÉ', 'Áî∞ÂæÑ', 'ËøêÂä®Âëò', 'Èî¶Ê†áËµõ', 'ÊØîËµõ', 'ËÅîËµõ', 'ÊØîËµõ', 'ÁêÉÂëò', 'ÁêÉÈòü', 'ÊïôÁªÉ', 'Â••Ëøê‰ºö', 'Â•ñÁâå', 'ËÉúÂà©', 'ÂÜ†ÂÜõ', 'ÊñáÂåñ', 'ÊñáÂåñÁöÑ', 'ÈÅó‰∫ß', 'ËäÇÊó•', 'ËàûËπà', 'Á´ûËµõ'],
                'ko': ['ÏòàÏà†', 'ÏòàÏà†Í∞Ä', 'ÏùåÏïÖ', 'ÏùåÏïÖÍ∞Ä', 'Í∑∏Î¶º', 'Ï°∞Í∞Å', 'Í∞§Îü¨Î¶¨', 'Î∞ïÎ¨ºÍ¥Ä', 'Ï†ÑÏãúÌöå', 'Í≥µÏó∞', 'Í∑πÏû•', 'ÏòÅÌôî', 'ÏãúÎÑ§Îßà', 'Ïä§Ìè¨Ï∏†', 'Ï∂ïÍµ¨', 'ÎÜçÍµ¨', 'ÌÖåÎãàÏä§', 'Ïú°ÏÉÅ', 'ÏÑ†Ïàò', 'ÏÑ†ÏàòÍ∂å', 'ÌÜ†ÎÑàÎ®ºÌä∏', 'Î¶¨Í∑∏', 'Í≤ΩÍ∏∞', 'Í≤åÏûÑ', 'ÏÑ†Ïàò', 'ÌåÄ', 'ÏΩîÏπò', 'Ïò¨Î¶ºÌîΩ', 'Î©îÎã¨', 'ÏäπÎ¶¨', 'Ï±îÌîºÏñ∏', 'Î¨∏Ìôî', 'Î¨∏ÌôîÏ†Å', 'Ïú†ÏÇ∞', 'Ï∂ïÏ†ú', 'Ï∂§', 'ÎåÄÌöå'],
                'es': ['arte', 'artista', 'm√∫sica', 'm√∫sico', 'pintura', 'escultura', 'galer√≠a', 'museo', 'exposici√≥n', 'actuaci√≥n', 'teatro', 'cine', 'pel√≠cula', 'deporte', 'deportes', 'f√∫tbol', 'baloncesto', 'tenis', 'atletismo', 'atleta', 'campeonato', 'torneo', 'liga', 'partido', 'juego', 'jugador', 'equipo', 'entrenador', 'ol√≠mpico', 'olimpiadas', 'medalla', 'victoria', 'campe√≥n', 'cultura', 'cultural', 'patrimonio', 'festival', 'danza', 'competici√≥n'],
                'pt': ['arte', 'artista', 'm√∫sica', 'm√∫sico', 'pintura', 'escultura', 'galeria', 'museu', 'exposi√ß√£o', 'espet√°culo', 'teatro', 'cinema', 'filme', 'esporte', 'esportes', 'futebol', 'basquete', 't√™nis', 'atletismo', 'atleta', 'campeonato', 'torneio', 'liga', 'partida', 'jogo', 'jogador', 'equipe', 'treinador', 'ol√≠mpico', 'olimp√≠adas', 'medalha', 'vit√≥ria', 'campe√£o', 'cultura', 'cultural', 'patrim√¥nio', 'festival', 'dan√ßa', 'competi√ß√£o']
            }
        }
    
    def preprocess_text(self, text, lang='en'):
        """Clean and normalize text for feature extraction"""
        if not isinstance(text, str):
            return ''
        
        text = text.lower()
        
        # Language-specific normalization
        if lang in {'en', 'fr', 'es', 'pt'}:
            text = self._strip_accents(text)
        elif lang == 'ar':
            text = self._normalize_arabic(text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def _strip_accents(self, s):
        return ''.join(c for c in unicodedata.normalize('NFKD', s) 
                      if not unicodedata.combining(c))
    
    def _normalize_arabic(self, s):
        s = re.sub('[\u064B-\u0652]', '', s)  # Remove diacritics
        s = s.replace('\u0649', '\u064A')     # Normalize alef/yeh
        return s
    
    def train(self, manual_df, keyword_df=None, keyword_confidence_threshold=0.5,
              text_cols=['title', 'description'], manual_label_col='topic_label',
              keyword_label_col='predicted_label_id', keyword_conf_col='prediction_confidence'):
        """
        Train the model on BOTH manual labels AND high-confidence keyword labels
        
        Parameters:
        -----------
        manual_df : DataFrame
            Your 200 manually labeled examples with ground truth
        keyword_df : DataFrame (optional)
            Output from your previous keyword labeling code
        keyword_confidence_threshold : float
            Only use keyword labels with confidence >= this threshold (default: 0.5)
        text_cols : list
            Text columns to combine for features
        manual_label_col : str
            Column name for manual ground truth labels (0-5)
        keyword_label_col : str
            Column name for keyword-predicted labels
        keyword_conf_col : str
            Column name for keyword confidence scores
        """
        print("="*80)
        print("TRAINING SEMI-SUPERVISED MODEL")
        print("="*80)
        
        # Process manual labels
        manual_df = manual_df.copy()
        if isinstance(text_cols, list):
            manual_df['combined_text'] = manual_df[text_cols].fillna('').agg(' '.join, axis=1)
        else:
            manual_df['combined_text'] = manual_df[text_cols].fillna('')
        
        if 'language_code' in manual_df.columns:
            manual_df['processed_text'] = manual_df.apply(
                lambda row: self.preprocess_text(row['combined_text'], row['language_code']),
                axis=1
            )
        else:
            manual_df['processed_text'] = manual_df['combined_text'].apply(self.preprocess_text)
        
        # Start with manual labels
        X_manual = manual_df['processed_text']
        y_manual = manual_df[manual_label_col]
        
        # Remove NaN labels
        valid_mask = y_manual.notna()
        X_manual = X_manual[valid_mask]
        y_manual = y_manual[valid_mask]
        
        print(f"\nüìä MANUAL LABELS: {len(X_manual)} examples")
        print(f"   Topic distribution:\n{y_manual.value_counts()}\n")
        
        # Add high-confidence keyword labels if provided
        if keyword_df is not None:
            print(f"üìä KEYWORD LABELS: Processing {len(keyword_df)} examples...")
            
            keyword_df = keyword_df.copy()
            
            # Filter for high confidence AND classified (not 'Unclassified')
            high_conf_mask = (
                (keyword_df[keyword_conf_col] >= keyword_confidence_threshold) &
                (keyword_df[keyword_label_col].notna()) &
                (keyword_df[keyword_label_col] != 'Unclassified')
            )
            
            keyword_df_filtered = keyword_df[high_conf_mask].copy()
            print(f"   Filtered to {len(keyword_df_filtered)} high-confidence examples "
                  f"(threshold: {keyword_confidence_threshold})")
            
            if len(keyword_df_filtered) > 0:
                # Process keyword data
                if isinstance(text_cols, list):
                    keyword_df_filtered['combined_text'] = keyword_df_filtered[text_cols].fillna('').agg(' '.join, axis=1)
                else:
                    keyword_df_filtered['combined_text'] = keyword_df_filtered[text_cols].fillna('')
                
                if 'language_code' in keyword_df_filtered.columns:
                    keyword_df_filtered['processed_text'] = keyword_df_filtered.apply(
                        lambda row: self.preprocess_text(row['combined_text'], row['language_code']),
                        axis=1
                    )
                else:
                    keyword_df_filtered['processed_text'] = keyword_df_filtered['combined_text'].apply(self.preprocess_text)
                
                X_keyword = keyword_df_filtered['processed_text']
                y_keyword = keyword_df_filtered[keyword_label_col]
                
                print(f"   Topic distribution:\n{y_keyword.value_counts()}\n")
                
                # Combine manual + keyword labels
                X_combined = pd.concat([X_manual, X_keyword], ignore_index=True)
                y_combined = pd.concat([y_manual, y_keyword], ignore_index=True)
                
                print(f"‚úÖ TOTAL TRAINING DATA: {len(X_combined)} examples")
                print(f"   - Manual: {len(X_manual)} ({len(X_manual)/len(X_combined)*100:.1f}%)")
                print(f"   - Keyword (high-conf): {len(X_keyword)} ({len(X_keyword)/len(X_combined)*100:.1f}%)")
            else:
                print("   ‚ö†Ô∏è  No high-confidence keyword labels found, using manual only")
                X_combined = X_manual
                y_combined = y_manual
        else:
            print("   ‚ÑπÔ∏è  No keyword data provided, using manual labels only")
            X_combined = X_manual
            y_combined = y_manual
        
        print(f"\nüìà COMBINED TOPIC DISTRIBUTION:")
        print(y_combined.value_counts())
        print()
        
        # Create pipeline: TF-IDF + Logistic Regression
        self.model = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),        # Unigrams and bigrams
                min_df=2,                   # Ignore rare terms
                max_df=0.8,                 # Ignore too common terms
                sublinear_tf=True           # Log scaling
            )),
            ('clf', LogisticRegression(
                max_iter=1000,
                class_weight='balanced',    # Handle class imbalance
                C=1.0,
                random_state=42
            ))
        ])
        
        # Cross-validation
        print("üîÑ Running 5-fold cross-validation...")
        cv_scores = cross_val_score(self.model, X_combined, y_combined, cv=5, scoring='accuracy')
        prec_scores = cross_val_score(self.model, X_combined, y_combined, cv=5, scoring='precision_weighted')
        print(f"   CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
        print(f"   CV Precision (weighted): {prec_scores.mean():.3f} (+/- {prec_scores.std():.3f})")
        
        # Train final model
        print("\nüéØ Training final model...")
        self.model.fit(X_combined, y_combined)
        self.trained = True
        
        # Training set performance
        y_pred = self.model.predict(X_combined)
        train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(y_combined, y_pred, average='weighted', zero_division=0)
        train_acc = (y_combined == y_pred).mean()
        print("\n" + "="*80)
        print("TRAINING SET PERFORMANCE")
        print("="*80)
        print(classification_report(y_combined, y_pred, 
                                   target_names=list(self.topic_labels.values()),
                                   zero_division=0))
        
        # Show which manual labels the model agrees/disagrees with
        if len(X_manual) > 0:
            y_manual_pred = self.model.predict(X_manual)
            manual_accuracy = (y_manual == y_manual_pred).mean()
            print(f"\n‚úÖ Agreement with manual labels: {manual_accuracy:.1%}")
            
            disagreements = (y_manual != y_manual_pred).sum()
            if disagreements > 0:
                print(f"   ‚ö†Ô∏è  {disagreements} disagreements - review these for quality check")
        self.metrics = {
            'cv_accuracy_mean': float(cv_scores.mean()),
            'cv_accuracy_std': float(cv_scores.std()),
            'cv_precision_mean': float(prec_scores.mean()),
            'cv_precision_std': float(prec_scores.std()),
            'train_accuracy': float(train_acc),
            'train_precision_weighted': float(train_prec)
        }
        
        return self
    
    def predict(self, df, text_cols=['title', 'description'], 
                confidence_threshold=0.5, use_keyword_fallback=True):
        """
        Predict topics for new articles
        
        Parameters:
        -----------
        confidence_threshold : float
                            If ML prediction confidence < threshold, use keyword fallback
        use_keyword_fallback : bool
            Whether to use keyword matching for low-confidence cases
        """
        if not self.trained:
            raise ValueError("Model not trained. Call train() first.")
        
        print("\n" + "="*80)
        print("PREDICTING TOPICS")
        print("="*80)
        
        df = df.copy()
        
        # Combine text
        if isinstance(text_cols, list):
            df['combined_text'] = df[text_cols].fillna('').agg(' '.join, axis=1)
        else:
            df['combined_text'] = df[text_cols].fillna('')
        
        # Preprocess
        if 'language_code' in df.columns:
            df['processed_text'] = df.apply(
                lambda row: self.preprocess_text(row['combined_text'], row['language_code']),
                axis=1
            )
        else:
            df['processed_text'] = df['combined_text'].apply(self.preprocess_text)
        
        # ML predictions
        print(f"ü§ñ Running ML predictions on {len(df)} articles...")
        X = df['processed_text']
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)
        confidences = probabilities.max(axis=1)
        
        df['predicted_label_id'] = predictions
        df['predicted_label_name'] = df['predicted_label_id'].map(self.topic_labels)
        df['prediction_confidence'] = confidences
        df['prediction_source'] = 'ml_model'  # Track prediction source
        
        # Keyword fallback for low-confidence predictions (vectorized to avoid chained assignment)
        if use_keyword_fallback:
            low_conf_mask = df['prediction_confidence'] < confidence_threshold
            n_low_conf = low_conf_mask.sum()
            
            if n_low_conf > 0:
                print(f"üîÑ Applying keyword fallback to {n_low_conf} low-confidence predictions...", flush=True)
                low_conf_df = df.loc[low_conf_mask].copy()
                updated = low_conf_df.apply(self._keyword_fallback, axis=1)
                cols_to_update = ['predicted_label_id', 'predicted_label_name', 'prediction_confidence', 'prediction_source']
                df.loc[updated.index, cols_to_update] = updated[cols_to_update].values
        
        print(f"\n‚úÖ Prediction complete!")
        print(f"   Average confidence: {df['prediction_confidence'].mean():.3f}")
        print(f"   Min confidence: {df['prediction_confidence'].min():.3f}")
        
        print(f"\nüìä PREDICTION SOURCE BREAKDOWN:")
        print(df['prediction_source'].value_counts())
        
        print(f"\nüìä TOPIC DISTRIBUTION:")
        print(df['predicted_label_name'].value_counts())
        
        return df
    
    def _keyword_fallback(self, row):
        """Apply keyword matching for a single row and return an updated row"""
        text = row.get('combined_text', '')
        if isinstance(text, str):
            text = text.lower()
        else:
            text = ''
        lang = row.get('language_code', 'en')
        
        topic_scores = {}
        for topic_name, lang_keywords in self.multilingual_keywords.items():
            keywords = lang_keywords.get(lang, lang_keywords.get('en', []))
            score = sum(1 for kw in keywords if kw in text)
            topic_scores[topic_name] = score
        
        if max(topic_scores.values()) > 0:
            best_topic = max(topic_scores, key=topic_scores.get)
            total = sum(topic_scores.values())
            confidence = topic_scores[best_topic] / (total + 1e-6)
            
            label_to_id = {v: k for k, v in self.topic_labels.items()}
            row['predicted_label_name'] = best_topic
            row['predicted_label_id'] = label_to_id[best_topic]
            row['prediction_confidence'] = confidence
            row['prediction_source'] = 'keyword_fallback'
        return row
    
    def save_model(self, path='topic_labeler_model.pkl'):
        """Save trained model to disk"""
        if not self.trained:
            raise ValueError("No trained model to save")
        joblib.dump(self.model, path)
        print(f"‚úÖ Model saved to {path}")
    
    def load_model(self, path='topic_labeler_model.pkl'):
        """Load trained model from disk"""
        self.model = joblib.load(path)
        self.trained = True
        print(f"‚úÖ Model loaded from {path}")
    
    def export_for_review(self, df, output_path, n_samples=100, 
                         strategy='lowest_confidence', include_disagreements=True):
        """
        Export articles for manual review to improve the model
        
        Parameters:
        -----------
        df : DataFrame
            Predictions from predict() method
        output_path : str/Path
            Where to save the CSV for review
        n_samples : int
            Number of samples to export
        strategy : str
            'lowest_confidence' - pick articles with lowest prediction confidence
            'random_stratified' - random sample from each topic
            'uncertainty_sampling' - articles where top 2 predictions are close
        include_disagreements : bool
            If df has 'old_label' column, prioritize disagreements
        """
        print("\n" + "="*80)
        print("EXPORTING ARTICLES FOR MANUAL REVIEW")
        print("="*80)
        
        df = df.copy()
        
        # Strategy 1: Prioritize disagreements with old labels
        if include_disagreements and 'old_label' in df.columns:
            disagreements = df[df['predicted_label_name'] != df['old_label']]
            if len(disagreements) > 0:
                n_disagree = min(n_samples // 3, len(disagreements))
                print(f"üìå Including {n_disagree} disagreements with old labels")
                review_set = disagreements.nlargest(n_disagree, 'prediction_confidence')
                remaining = n_samples - n_disagree
            else:
                review_set = pd.DataFrame()
                remaining = n_samples
        else:
            review_set = pd.DataFrame()
            remaining = n_samples
        
        # Strategy 2: Select based on chosen strategy
        available_df = df[~df.index.isin(review_set.index)]
        
        if strategy == 'lowest_confidence':
            print(f"üìå Selecting {remaining} lowest-confidence predictions")
            selected = available_df.nsmallest(remaining, 'prediction_confidence')
        
        elif strategy == 'uncertainty_sampling':
            print(f"üìå Selecting {remaining} most uncertain predictions (top 2 classes close)")
            # Get prediction probabilities if available
            if hasattr(self.model, 'predict_proba'):
                X = available_df['processed_text']
                probs = self.model.predict_proba(X)
                # Calculate margin between top 2 predictions
                sorted_probs = np.sort(probs, axis=1)
                margins = sorted_probs[:, -1] - sorted_probs[:, -2]
                available_df['uncertainty_margin'] = margins
                selected = available_df.nsmallest(remaining, 'uncertainty_margin')
            else:
                # Fallback to lowest confidence
                selected = available_df.nsmallest(remaining, 'prediction_confidence')
        
        elif strategy == 'random_stratified':
            print(f"üìå Selecting {remaining} random samples (stratified by topic)")
            # Sample proportionally from each topic
            selected = available_df.groupby('predicted_label_name', group_keys=False).apply(
                lambda x: x.sample(min(len(x), max(1, remaining * len(x) // len(available_df))))
            ).head(remaining)
        
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
        
        # Combine
        review_set = pd.concat([review_set, selected])
        
        # Prepare output columns
        output_cols = ['title', 'description', 'predicted_label_name', 
                      'prediction_confidence', 'prediction_source']
        
        # Add language if available
        if 'language_code' in review_set.columns:
            output_cols.insert(0, 'language_code')
        
        # Add old label for comparison if available
        if 'old_label' in review_set.columns:
            output_cols.append('old_label')
        
        # Add URL if available
        if 'url' in review_set.columns:
            output_cols.append('url')
        
        # Add empty column for manual correction
        review_set['manual_label'] = ''
        review_set['review_notes'] = ''
        output_cols.extend(['manual_label', 'review_notes'])
        
        # Filter to available columns
        output_cols = [col for col in output_cols if col in review_set.columns]
        
        # Save
        review_set[output_cols].to_csv(output_path, index=False)
        
        print(f"\n‚úÖ Exported {len(review_set)} articles to {output_path}")
        print(f"\nüìä REVIEW SET BREAKDOWN:")
        print(review_set['predicted_label_name'].value_counts())
        print(f"\nüí° Instructions:")
        print(f"   1. Open {output_path}")
        print(f"   2. Fill in 'manual_label' column with correct topic (0-5)")
        print(f"   3. Optionally add notes in 'review_notes' column")
        print(f"   4. Save and use in next training iteration")
        
        return review_set
        ax1.axvline(df['prediction_confidence'].median(), color='orange', linestyle='--',
                   label=f'Median: {df["prediction_confidence"].median():.3f}')
        ax1.set_xlabel('Prediction Confidence')
        ax1.set_ylabel('Number of Articles')
        ax1.set_title('Overall Confidence Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Confidence by Topic (Boxplot)
        ax2 = axes[0, 1]
        topic_order = df.groupby('predicted_label_name')['prediction_confidence'].median().sort_values(ascending=False).index
        sns.boxplot(data=df, y='predicted_label_name', x='prediction_confidence', 
                   order=topic_order, ax=ax2, palette='Set2')
        ax2.set_xlabel('Prediction Confidence')
        ax2.set_ylabel('Topic')
        ax2.set_title('Confidence Distribution by Topic')
        ax2.grid(True, alpha=0.3, axis='x')
        
        # 3. Topic Distribution (Bar Chart)
        ax3 = axes[0, 2]
        topic_counts = df['predicted_label_name'].value_counts()
        colors = sns.color_palette('Set3', len(topic_counts))
        topic_counts.plot(kind='bar', ax=ax3, color=colors, edgecolor='black')
        ax3.set_xlabel('Topic')
        ax3.set_ylabel('Number of Articles')
        ax3.set_title('Article Distribution by Topic')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3, axis='y')
        
        # Add counts on bars
        for i, v in enumerate(topic_counts):
            ax3.text(i, v + max(topic_counts)*0.01, str(v), ha='center', va='bottom', fontweight='bold')
        
        # 4. Confidence Thresholds Analysis
        ax4 = axes[1, 0]
        thresholds = np.arange(0, 1.01, 0.05)
        articles_above = [((df['prediction_confidence'] >= t).sum()) for t in thresholds]
        ax4.plot(thresholds, articles_above, marker='o', linewidth=2, markersize=4, color='darkgreen')
        ax4.fill_between(thresholds, articles_above, alpha=0.3, color='lightgreen')
        ax4.set_xlabel('Confidence Threshold')
        ax4.set_ylabel('Number of Articles')
        ax4.set_title('Articles Above Confidence Threshold')
        ax4.grid(True, alpha=0.3)
        
        # Add reference lines
        for threshold, label in [(0.4, '0.4'), (0.5, '0.5'), (0.7, '0.7')]:
            count = (df['prediction_confidence'] >= threshold).sum()
            ax4.axvline(threshold, color='red', linestyle='--', alpha=0.5)
            ax4.text(threshold, max(articles_above)*0.95, 
                    f'{threshold}\n({count})', ha='center', fontsize=8)
        
        # 5. Prediction Source Breakdown (if available)
        ax5 = axes[1, 1]
        if 'prediction_source' in df.columns:
            source_counts = df['prediction_source'].value_counts()
            colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(source_counts)]
            wedges, texts, autotexts = ax5.pie(source_counts, labels=source_counts.index, 
                                               autopct='%1.1f%%', colors=colors,
                                               startangle=90, textprops={'fontsize': 10})
            ax5.set_title('Prediction Source Distribution')
            # Make percentage text bold
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
        else:
            ax5.text(0.5, 0.5, 'No prediction source data', ha='center', va='center', fontsize=12)
            ax5.axis('off')
        
        # 6. Confidence vs Topic Heatmap
        ax6 = axes[1, 2]
        confidence_bins = pd.cut(df['prediction_confidence'], bins=[0, 0.4, 0.6, 0.8, 1.0],
                                labels=['Low\n(0-0.4)', 'Medium\n(0.4-0.6)', 
                                       'High\n(0.6-0.8)', 'Very High\n(0.8-1.0)'])
        heatmap_data = pd.crosstab(df['predicted_label_name'], confidence_bins)
        sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlOrRd', ax=ax6, 
                   cbar_kws={'label': 'Article Count'})
        ax6.set_xlabel('Confidence Level')
        ax6.set_ylabel('Topic')
        ax6.set_title('Topic vs Confidence Level Heatmap')
        
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            Path(save_path).mkdir(parents=True, exist_ok=True)
            plot_file = Path(save_path) / 'prediction_analysis.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Saved visualization to {plot_file}")
        
        if show:
            plt.show()
        else:
            plt.close(fig)
        
        # Print summary statistics
        print("\n" + "="*80)
        print("CONFIDENCE STATISTICS")
        print("="*80)
        print(f"Mean confidence: {df['prediction_confidence'].mean():.3f}")
        print(f"Median confidence: {df['prediction_confidence'].median():.3f}")
        print(f"Std deviation: {df['prediction_confidence'].std():.3f}")
        print(f"\nArticles by confidence level:")
        print(f"  Very High (>0.8): {(df['prediction_confidence'] > 0.8).sum()} ({(df['prediction_confidence'] > 0.8).sum()/len(df)*100:.1f}%)")
        print(f"  High (0.6-0.8): {((df['prediction_confidence'] >= 0.6) & (df['prediction_confidence'] <= 0.8)).sum()} ({((df['prediction_confidence'] >= 0.6) & (df['prediction_confidence'] <= 0.8)).sum()/len(df)*100:.1f}%)")
        print(f"  Medium (0.4-0.6): {((df['prediction_confidence'] >= 0.4) & (df['prediction_confidence'] < 0.6)).sum()} ({((df['prediction_confidence'] >= 0.4) & (df['prediction_confidence'] < 0.6)).sum()/len(df)*100:.1f}%)")
        print(f"  Low (<0.4): {(df['prediction_confidence'] < 0.4).sum()} ({(df['prediction_confidence'] < 0.4).sum()/len(df)*100:.1f}%)")

    def visualize_predictions(self, df, save_path=None, show=False):
        """
        Create visualizations of prediction quality
        
        Parameters:
        -----------
        df : DataFrame
            Results from predict() method
        save_path : str/Path (optional)
            If provided, save plots to this directory
        """
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
        except ImportError:
            print("‚ö†Ô∏è  matplotlib/seaborn not installed. Run: pip install matplotlib seaborn")
            return
        
        print("\n" + "="*80)
        print("GENERATING VISUALIZATIONS")
        print("="*80)
        
        # Set style
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (15, 10)
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Topic Classification Analysis', fontsize=16, fontweight='bold')
        
        # 1. Confidence Distribution (Overall)
        ax1 = axes[0, 0]
        ax1.hist(df['prediction_confidence'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)
        ax1.axvline(df['prediction_confidence'].mean(), color='red', linestyle='--', 
                   label=f'Mean: {df["prediction_confidence"].mean():.3f}')
        ax1.axvline(df['prediction_confidence'].median(), color='orange', linestyle='--',
                   label=f'Median: {df["prediction_confidence"].median():.3f}')
        ax1.set_xlabel('Prediction Confidence')
        ax1.set_ylabel('Number of Articles')
        ax1.set_title('Overall Confidence Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Confidence by Topic (Boxplot)
        ax2 = axes[0, 1]
        topic_order = df.groupby('predicted_label_name')['prediction_confidence'].median().sort_values(ascending=False).index
        sns.boxplot(data=df, y='predicted_label_name', x='prediction_confidence', 
                   order=topic_order, ax=ax2, palette='Set2')
        ax2.set_xlabel('Prediction Confidence')
        ax2.set_ylabel('Topic')
        ax2.set_title('Confidence Distribution by Topic')
        ax2.grid(True, alpha=0.3, axis='x')
        
        # 3. Topic Distribution (Bar Chart)
        ax3 = axes[0, 2]
        topic_counts = df['predicted_label_name'].value_counts()
        colors = sns.color_palette('Set3', len(topic_counts))
        topic_counts.plot(kind='bar', ax=ax3, color=colors, edgecolor='black')
        ax3.set_xlabel('Topic')
        ax3.set_ylabel('Number of Articles')
        ax3.set_title('Article Distribution by Topic')
        ax3.tick_params(axis='x', rotation=45)
        ax3.grid(True, alpha=0.3, axis='y')
        
        # Add counts on bars
        for i, v in enumerate(topic_counts):
            ax3.text(i, v + max(topic_counts)*0.01, str(v), ha='center', va='bottom', fontweight='bold')
        
        # 4. Confidence Thresholds Analysis
        ax4 = axes[1, 0]
        thresholds = np.arange(0, 1.01, 0.05)
        articles_above = [((df['prediction_confidence'] >= t).sum()) for t in thresholds]
        ax4.plot(thresholds, articles_above, marker='o', linewidth=2, markersize=4, color='darkgreen')
        ax4.fill_between(thresholds, articles_above, alpha=0.3, color='lightgreen')
        ax4.set_xlabel('Confidence Threshold')
        ax4.set_ylabel('Number of Articles')
        ax4.set_title('Articles Above Confidence Threshold')
        ax4.grid(True, alpha=0.3)
        
        # Add reference lines
        for threshold, label in [(0.4, '0.4'), (0.5, '0.5'), (0.7, '0.7')]:
            count = (df['prediction_confidence'] >= threshold).sum()
            ax4.axvline(threshold, color='red', linestyle='--', alpha=0.5)
            ax4.text(threshold, max(articles_above)*0.95, 
                    f'{threshold}\n({count})', ha='center', fontsize=8)
        
        # 5. Prediction Source Breakdown (if available)
        ax5 = axes[1, 1]
        if 'prediction_source' in df.columns:
            source_counts = df['prediction_source'].value_counts()
            colors = ['#2ecc71', '#e74c3c', '#3498db'][:len(source_counts)]
            wedges, texts, autotexts = ax5.pie(source_counts, labels=source_counts.index, 
                                               autopct='%1.1f%%', colors=colors,
                                               startangle=90, textprops={'fontsize': 10})
            ax5.set_title('Prediction Source Distribution')
            # Make percentage text bold
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
        else:
            ax5.text(0.5, 0.5, 'No prediction source data', ha='center', va='center', fontsize=12)
            ax5.axis('off')
        
        # 6. Confidence vs Topic Heatmap
        ax6 = axes[1, 2]
        confidence_bins = pd.cut(df['prediction_confidence'], bins=[0, 0.4, 0.6, 0.8, 1.0],
                                labels=['Low\n(0-0.4)', 'Medium\n(0.4-0.6)', 
                                       'High\n(0.6-0.8)', 'Very High\n(0.8-1.0)'])
        heatmap_data = pd.crosstab(df['predicted_label_name'], confidence_bins)
        sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlOrRd', ax=ax6, 
                   cbar_kws={'label': 'Article Count'})
        ax6.set_xlabel('Confidence Level')
        ax6.set_ylabel('Topic')
        ax6.set_title('Topic vs Confidence Level Heatmap')
        
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            Path(save_path).mkdir(parents=True, exist_ok=True)
            plot_file = Path(save_path) / 'prediction_analysis.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Saved visualization to {plot_file}")
        
        if show:
            plt.show()
        else:
            plt.close(fig)
        
        # Print summary statistics
        print("\n" + "="*80)
        print("CONFIDENCE STATISTICS")
        print("="*80)
        print(f"Mean confidence: {df['prediction_confidence'].mean():.3f}")
        print(f"Median confidence: {df['prediction_confidence'].median():.3f}")
        print(f"Std deviation: {df['prediction_confidence'].std():.3f}")
        print(f"\nArticles by confidence level:")
        print(f"  Very High (>0.8): {(df['prediction_confidence'] > 0.8).sum()} ({(df['prediction_confidence'] > 0.8).sum()/len(df)*100:.1f}%)")
        print(f"  High (0.6-0.8): {((df['prediction_confidence'] >= 0.6) & (df['prediction_confidence'] <= 0.8)).sum()} ({((df['prediction_confidence'] >= 0.6) & (df['prediction_confidence'] <= 0.8)).sum()/len(df)*100:.1f}%)")
        print(f"  Medium (0.4-0.6): {((df['prediction_confidence'] >= 0.4) & (df['prediction_confidence'] < 0.6)).sum()} ({((df['prediction_confidence'] >= 0.4) & (df['prediction_confidence'] < 0.6)).sum()/len(df)*100:.1f}%)")
        print(f"  Low (<0.4): {(df['prediction_confidence'] < 0.4).sum()} ({(df['prediction_confidence'] < 0.4).sum()/len(df)*100:.1f}%)")


def save_metrics(model_name, metrics, metrics_path):
    metrics_path = Path(metrics_path)
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    row = {
        'model': model_name,
        'cv_accuracy_mean': metrics.get('cv_accuracy_mean'),
        'cv_accuracy_std': metrics.get('cv_accuracy_std'),
        'cv_precision_mean': metrics.get('cv_precision_mean'),
        'cv_precision_std': metrics.get('cv_precision_std'),
        'train_accuracy': metrics.get('train_accuracy'),
        'train_precision_weighted': metrics.get('train_precision_weighted')
    }
    try:
        if metrics_path.exists():
            df = pd.read_csv(metrics_path)
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(metrics_path, index=False)
        print(f"\n‚úÖ Metrics saved to {metrics_path}")
    except Exception as e:
        print(f"\n‚ùå Error saving metrics: {e}")


# ============================================================================
# USAGE EXAMPLE - COMBINING MANUAL + KEYWORD LABELS
# ============================================================================

if __name__ == "__main__":
    # Paths
    DATA_DIR = Path(__file__).parent.parent / "datascience" / "data_africa"
    
    MANUAL_LABELED_FILE = DATA_DIR / "manual_training_set.csv"  # Your 200 manual labels
    KEYWORD_LABELED_FILE = DATA_DIR / "all_languages_labeled.csv"  # Output from old code
    UNLABELED_FILE = DATA_DIR / "all_languages_links_ok.csv"
    OUTPUT_FILE = DATA_DIR / "ldamulti.csv"
    REVIEW_FILE = DATA_DIR / "for_manual_review.csv"
    VIZ_DIR = DATA_DIR / "visualizations"
    
    # ========================================================================
    # STEP 1: Load manually labeled data (200 examples)
    # ========================================================================
    print("üìÇ Loading manually labeled data...")
    manual_df = pd.read_csv(MANUAL_LABELED_FILE)
    # Expected columns: text, language_code, label (0-5)
    # Normalize manual text column name for downstream processing
    if 'text' not in manual_df.columns:
        raise ValueError("manual_training_set.csv must contain a 'text' column")
    manual_df['text'] = manual_df['text'].fillna('')
    print(f"   Loaded {len(manual_df)} manual labels\n")
    
    # ========================================================================
    # STEP 2: Load keyword-labeled data from your previous code
    # ========================================================================
    print("üìÇ Loading keyword-labeled data...")
    keyword_df = pd.read_csv(KEYWORD_LABELED_FILE)
    # Expected columns: title, description, language_code, 
    #                   predicted_label_id, predicted_label_name, prediction_confidence
    keyword_df['text'] = keyword_df[['title', 'description']].fillna('').agg(' '.join, axis=1)
    print(f"   Loaded {len(keyword_df)} keyword labels")
    print(f"   Confidence stats: mean={keyword_df['prediction_confidence'].mean():.3f}, "
          f"median={keyword_df['prediction_confidence'].median():.3f}\n")
    
    # ========================================================================
    # STEP 3: Train model on BOTH manual + high-confidence keyword labels
    # ========================================================================
    labeler = SupervisedTopicLabeler()
    
    # Try different thresholds to see what works best
    # Higher threshold = fewer but higher quality keyword labels
    # Lower threshold = more training data but potentially noisier
    KEYWORD_CONFIDENCE_THRESHOLD = 0.5  # Adjust this (try 0.4, 0.5, 0.6)
    
    labeler.train(
        manual_df=manual_df,
        keyword_df=keyword_df,
        keyword_confidence_threshold=KEYWORD_CONFIDENCE_THRESHOLD,
        text_cols=['text'],
        manual_label_col='label',  # Manual ground truth column
        keyword_label_col='predicted_label_id',  # From old code
        keyword_conf_col='prediction_confidence'  # From old code
    )
    
    # ========================================================================
    # STEP 4: Save model for reuse
    # ========================================================================
    labeler.save_model(DATA_DIR / 'topic_model_semisupervised.pkl')
    
    # ========================================================================
    # STEP 5: Predict on all data (or new unlabeled data)
    # ========================================================================
    print(f"\nüìÇ Loading data to label from {UNLABELED_FILE}...")
    unlabeled_df = pd.read_csv(UNLABELED_FILE)
    
    results_df = labeler.predict(
        unlabeled_df,
        text_cols=['title', 'description'],
        confidence_threshold=0.5,      # Requested threshold
        use_keyword_fallback=True
    )
    
    # ========================================================================
    # STEP 6: Save results
    # ========================================================================
    results_df.to_csv(OUTPUT_FILE, index=False)
    print(f"\n‚úÖ Labeled data saved to {OUTPUT_FILE}")
    
    # ========================================================================
    # STEP 7: Compare with original keyword labels
    # ========================================================================
    if 'predicted_label_id' in keyword_df.columns:
        print("\n" + "="*80)
        print("COMPARISON: NEW MODEL vs ORIGINAL KEYWORD LABELS")
        print("="*80)
        
        # Merge on a unique identifier (adjust if you have a different ID column)
        # Assuming row order is preserved
        comparison = results_df[['predicted_label_name', 'prediction_confidence']].copy()
        comparison.columns = ['new_label', 'new_confidence']
        comparison['old_label'] = keyword_df['predicted_label_name'].values[:len(comparison)]
        comparison['old_confidence'] = keyword_df['prediction_confidence'].values[:len(comparison)]
        
        # Add to results for export
        results_df['old_label'] = comparison['old_label']
        
        # Check agreement
        comparison['labels_match'] = comparison['new_label'] == comparison['old_label']
        agreement_rate = comparison['labels_match'].mean()
        
        print(f"\nüìä Agreement with original keyword labels: {agreement_rate:.1%}")
        print(f"   ({comparison['labels_match'].sum()} / {len(comparison)} articles)")
        
        # Show disagreements
        disagreements = comparison[~comparison['labels_match']]
        if len(disagreements) > 0:
            print(f"\n‚ö†Ô∏è  {len(disagreements)} disagreements found")
            print("\nTop 10 disagreements:")
            print(disagreements[['old_label', 'new_label', 'old_confidence', 'new_confidence']].head(10))
    
    # ========================================================================
    # STEP 8: Generate visualizations
    # ========================================================================
    labeler.visualize_predictions(results_df, save_path=VIZ_DIR, show=False)
    
    # ========================================================================
    # STEP 9: Export articles for manual review (iterative improvement)
    # ========================================================================
    review_df = labeler.export_for_review(
        results_df,
        output_path=REVIEW_FILE,
        n_samples=100,  # Number of articles to review
        strategy='lowest_confidence',  # Options: 'lowest_confidence', 'uncertainty_sampling', 'random_stratified'
        include_disagreements=True  # Prioritize disagreements with old labels
    )

    save_metrics(
        model_name="ldamulti_it-idf",
        metrics=labeler.metrics,
        metrics_path=DATA_DIR / "model_metrics.csv"
    )
    
    # ========================================================================
    # STEP 10: Instructions for iterative improvement
    # ========================================================================
    print("\n" + "="*80)
    print("üîÑ ITERATIVE IMPROVEMENT WORKFLOW")
    print("="*80)
    print(f"""
1. Review the exported file: {REVIEW_FILE}
2. Fill in the 'manual_label' column with correct labels (0-5)
3. Save the reviewed file
4. Load it and combine with your existing manual labels:
   
   ```python
   # Load your new reviews
   new_reviews = pd.read_csv('{REVIEW_FILE}')
   new_reviews = new_reviews[new_reviews['manual_label'].notna()]
   
   # Combine with existing manual labels
   existing_manual = pd.read_csv('{MANUAL_LABELED_FILE}')
   combined_manual = pd.concat([existing_manual, new_reviews])
   combined_manual.to_csv('{MANUAL_LABELED_FILE}', index=False)
   
   # Retrain the model
   labeler = SupervisedTopicLabeler()
   labeler.train(manual_df=combined_manual, keyword_df=keyword_df)
   ```

5. Repeat this process until prediction quality is satisfactory

üìä CURRENT STATUS:
   - Training data: {len(manual_df)} manual + high-confidence keyword labels
   - Unlabeled articles: {len(results_df)}
   - Ready for review: {len(review_df)}
   - Visualizations saved to: {VIZ_DIR}
    """)